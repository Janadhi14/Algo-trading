{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf \n",
    "import tensorflow as tf\n",
    "import sklearn \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>7.611786</td>\n",
       "      <td>7.619643</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>7.526071</td>\n",
       "      <td>6.434925</td>\n",
       "      <td>352410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>7.643214</td>\n",
       "      <td>6.535086</td>\n",
       "      <td>493729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>6.546384</td>\n",
       "      <td>601904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>7.534643</td>\n",
       "      <td>6.442254</td>\n",
       "      <td>552160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>7.520714</td>\n",
       "      <td>6.430345</td>\n",
       "      <td>477131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>136.820007</td>\n",
       "      <td>138.589996</td>\n",
       "      <td>135.630005</td>\n",
       "      <td>138.270004</td>\n",
       "      <td>138.270004</td>\n",
       "      <td>72433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>139.899994</td>\n",
       "      <td>141.910004</td>\n",
       "      <td>139.770004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>89116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>142.699997</td>\n",
       "      <td>143.490005</td>\n",
       "      <td>140.970001</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>70207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>142.130005</td>\n",
       "      <td>143.419998</td>\n",
       "      <td>137.320007</td>\n",
       "      <td>137.440002</td>\n",
       "      <td>137.440002</td>\n",
       "      <td>67083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-29</th>\n",
       "      <td>137.460007</td>\n",
       "      <td>140.669998</td>\n",
       "      <td>136.669998</td>\n",
       "      <td>139.229996</td>\n",
       "      <td>139.229996</td>\n",
       "      <td>66242400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3145 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2009-12-31    7.611786    7.619643    7.520000    7.526071    6.434925   \n",
       "2010-01-04    7.622500    7.660714    7.585000    7.643214    6.535086   \n",
       "2010-01-05    7.664286    7.699643    7.616071    7.656429    6.546384   \n",
       "2010-01-06    7.656429    7.686786    7.526786    7.534643    6.442254   \n",
       "2010-01-07    7.562500    7.571429    7.466071    7.520714    6.430345   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-06-23  136.820007  138.589996  135.630005  138.270004  138.270004   \n",
       "2022-06-24  139.899994  141.910004  139.770004  141.660004  141.660004   \n",
       "2022-06-27  142.699997  143.490005  140.970001  141.660004  141.660004   \n",
       "2022-06-28  142.130005  143.419998  137.320007  137.440002  137.440002   \n",
       "2022-06-29  137.460007  140.669998  136.669998  139.229996  139.229996   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2009-12-31  352410800  \n",
       "2010-01-04  493729600  \n",
       "2010-01-05  601904800  \n",
       "2010-01-06  552160000  \n",
       "2010-01-07  477131200  \n",
       "...               ...  \n",
       "2022-06-23   72433800  \n",
       "2022-06-24   89116800  \n",
       "2022-06-27   70207900  \n",
       "2022-06-28   67083400  \n",
       "2022-06-29   66242400  \n",
       "\n",
       "[3145 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple = yf.download(\"AAPL\", start = \"2010-01-01\", end =\"2022-07-01\" )\n",
    "# creating a variable called apple and then downloading using yahoo finiance \n",
    "Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>7.611786</td>\n",
       "      <td>7.619643</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>7.526071</td>\n",
       "      <td>6.434925</td>\n",
       "      <td>352410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>7.643214</td>\n",
       "      <td>6.535086</td>\n",
       "      <td>493729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>6.546384</td>\n",
       "      <td>601904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>7.534643</td>\n",
       "      <td>6.442254</td>\n",
       "      <td>552160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>7.520714</td>\n",
       "      <td>6.430345</td>\n",
       "      <td>477131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23</th>\n",
       "      <td>136.820007</td>\n",
       "      <td>138.589996</td>\n",
       "      <td>135.630005</td>\n",
       "      <td>138.270004</td>\n",
       "      <td>138.270004</td>\n",
       "      <td>72433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24</th>\n",
       "      <td>139.899994</td>\n",
       "      <td>141.910004</td>\n",
       "      <td>139.770004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>89116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27</th>\n",
       "      <td>142.699997</td>\n",
       "      <td>143.490005</td>\n",
       "      <td>140.970001</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>141.660004</td>\n",
       "      <td>70207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-28</th>\n",
       "      <td>142.130005</td>\n",
       "      <td>143.419998</td>\n",
       "      <td>137.320007</td>\n",
       "      <td>137.440002</td>\n",
       "      <td>137.440002</td>\n",
       "      <td>67083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-29</th>\n",
       "      <td>137.460007</td>\n",
       "      <td>140.669998</td>\n",
       "      <td>136.669998</td>\n",
       "      <td>139.229996</td>\n",
       "      <td>139.229996</td>\n",
       "      <td>66242400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3145 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2009-12-31    7.611786    7.619643    7.520000    7.526071    6.434925   \n",
       "2010-01-04    7.622500    7.660714    7.585000    7.643214    6.535086   \n",
       "2010-01-05    7.664286    7.699643    7.616071    7.656429    6.546384   \n",
       "2010-01-06    7.656429    7.686786    7.526786    7.534643    6.442254   \n",
       "2010-01-07    7.562500    7.571429    7.466071    7.520714    6.430345   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-06-23  136.820007  138.589996  135.630005  138.270004  138.270004   \n",
       "2022-06-24  139.899994  141.910004  139.770004  141.660004  141.660004   \n",
       "2022-06-27  142.699997  143.490005  140.970001  141.660004  141.660004   \n",
       "2022-06-28  142.130005  143.419998  137.320007  137.440002  137.440002   \n",
       "2022-06-29  137.460007  140.669998  136.669998  139.229996  139.229996   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2009-12-31  352410800  \n",
       "2010-01-04  493729600  \n",
       "2010-01-05  601904800  \n",
       "2010-01-06  552160000  \n",
       "2010-01-07  477131200  \n",
       "...               ...  \n",
       "2022-06-23   72433800  \n",
       "2022-06-24   89116800  \n",
       "2022-06-27   70207900  \n",
       "2022-06-28   67083400  \n",
       "2022-06-29   66242400  \n",
       "\n",
       "[3145 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to get just the close and then see if there is a relation with the volume \n",
    "Apple.loc[:,\"Close\"].to_frame()\n",
    "Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-31</th>\n",
       "      <td>7.611786</td>\n",
       "      <td>7.619643</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>7.526071</td>\n",
       "      <td>6.434925</td>\n",
       "      <td>352410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>7.643214</td>\n",
       "      <td>6.535086</td>\n",
       "      <td>493729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>6.546384</td>\n",
       "      <td>601904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>7.534643</td>\n",
       "      <td>6.442254</td>\n",
       "      <td>552160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>7.520714</td>\n",
       "      <td>6.430345</td>\n",
       "      <td>477131200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close     Volume\n",
       "Date                                                                    \n",
       "2009-12-31  7.611786  7.619643  7.520000  7.526071   6.434925  352410800\n",
       "2010-01-04  7.622500  7.660714  7.585000  7.643214   6.535086  493729600\n",
       "2010-01-05  7.664286  7.699643  7.616071  7.656429   6.546384  601904800\n",
       "2010-01-06  7.656429  7.686786  7.526786  7.534643   6.442254  552160000\n",
       "2010-01-07  7.562500  7.571429  7.466071  7.520714   6.430345  477131200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we dont need this becuase we are only looking at quantative data \n",
    "Apple_one_hot = pd.get_dummies(Apple)\n",
    "Apple_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a datframe to examine if there is a relation to the percentage change in the prices \n",
    "Apple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                Open      High       Low     Close  Adj Close\n",
       " Date                                                         \n",
       " 2009-12-31  7.611786  7.619643  7.520000  7.526071   6.434925\n",
       " 2010-01-04  7.622500  7.660714  7.585000  7.643214   6.535086\n",
       " 2010-01-05  7.664286  7.699643  7.616071  7.656429   6.546384\n",
       " 2010-01-06  7.656429  7.686786  7.526786  7.534643   6.442254\n",
       " 2010-01-07  7.562500  7.571429  7.466071  7.520714   6.430345,\n",
       " Date\n",
       " 2009-12-31    352410800\n",
       " 2010-01-04    493729600\n",
       " 2010-01-05    601904800\n",
       " 2010-01-06    552160000\n",
       " 2010-01-07    477131200\n",
       " Name: Volume, dtype: int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Apple.drop(\"Volume\", axis = 1)\n",
    "y = Apple[\"Volume\"]\n",
    "X.head(), y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145, 2516, 629)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)\n",
    "len(X), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                  Open        High         Low       Close   Adj Close\n",
       " Date                                                                  \n",
       " 2020-07-23   96.997498   97.077499   92.010002   92.845001   91.699692\n",
       " 2013-01-10   18.876785   18.882856   18.411428   18.696787   16.128056\n",
       " 2015-12-03   29.137501   29.197500   28.555000   28.799999   26.446291\n",
       " 2014-11-17   28.567499   29.320000   28.325001   28.497499   25.730120\n",
       " 2010-11-16   10.918571   10.985714   10.690000   10.771071    9.209456\n",
       " ...                ...         ...         ...         ...         ...\n",
       " 2022-04-13  167.389999  171.039993  166.770004  170.399994  170.149994\n",
       " 2014-05-09   20.876429   20.937500   20.726070   20.912144   18.706797\n",
       " 2014-06-30   23.025000   23.432501   23.022499   23.232500   20.782465\n",
       " 2015-02-24   33.235001   33.400002   32.792500   33.042500   29.951494\n",
       " 2013-06-04   16.186428   16.229643   15.978214   16.046785   14.014961\n",
       " \n",
       " [2516 rows x 5 columns],\n",
       "                   Open        High         Low       Close   Adj Close\n",
       " Date                                                                  \n",
       " 2021-02-03  135.759995  135.770004  133.610001  133.940002  132.754913\n",
       " 2021-10-21  148.809998  149.639999  147.869995  149.479996  148.853531\n",
       " 2014-03-26   19.518572   19.607143   19.245001   19.277857   17.149080\n",
       " 2017-11-21   42.695000   43.424999   42.695000   43.285000   41.285236\n",
       " 2021-11-17  151.000000  155.000000  150.990005  153.490005  153.069794\n",
       " ...                ...         ...         ...         ...         ...\n",
       " 2012-08-28   24.106428   24.146429   23.952499   24.100000   20.694403\n",
       " 2021-04-12  132.520004  132.850006  130.630005  131.240005  130.273178\n",
       " 2021-03-17  124.050003  125.860001  122.339996  124.760002  123.840912\n",
       " 2020-07-17   96.987503   97.147499   95.839996   96.327499   95.139221\n",
       " 2022-06-08  148.580002  149.869995  147.460007  147.960007  147.960007\n",
       " \n",
       " [629 rows x 5 columns])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 10:46:02.767773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-17 10:46:02.769163: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-07-17 10:46:03.123451: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/79 [================>.............] - ETA: 0s - loss: 238550016.0000 - mae: 238550016.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 10:46:03.422212: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 2ms/step - loss: 231714224.0000 - mae: 231714224.0000\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 224041792.0000 - mae: 224041792.0000\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 224011248.0000 - mae: 224011248.0000\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 222462480.0000 - mae: 222462480.0000\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 222800752.0000 - mae: 222800752.0000\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 220984688.0000 - mae: 220984688.0000\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 221881632.0000 - mae: 221881632.0000\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 220557792.0000 - mae: 220557792.0000\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 220036864.0000 - mae: 220036864.0000\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 218297296.0000 - mae: 218297296.0000\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 217860320.0000 - mae: 217860304.0000\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 216465392.0000 - mae: 216465392.0000\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 217176912.0000 - mae: 217176912.0000\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 213679088.0000 - mae: 213679088.0000\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 215449440.0000 - mae: 215449440.0000\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 215188080.0000 - mae: 215188080.0000\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 213784272.0000 - mae: 213784272.0000\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 215460112.0000 - mae: 215460112.0000\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 212327264.0000 - mae: 212327264.0000\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 213058416.0000 - mae: 213058416.0000\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 208944480.0000 - mae: 208944480.0000\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 210441104.0000 - mae: 210441104.0000\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 210389776.0000 - mae: 210389776.0000\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 209848672.0000 - mae: 209848672.0000\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 207898768.0000 - mae: 207898768.0000\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 209297952.0000 - mae: 209297952.0000\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 206700480.0000 - mae: 206700480.0000\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 207310112.0000 - mae: 207310112.0000\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 205098304.0000 - mae: 205098304.0000\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 203890480.0000 - mae: 203890480.0000\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 203545712.0000 - mae: 203545712.0000\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 204123936.0000 - mae: 204123936.0000\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 202368656.0000 - mae: 202368656.0000\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 205643040.0000 - mae: 205643040.0000\n",
      "Epoch 35/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 202249936.0000 - mae: 202249936.0000\n",
      "Epoch 36/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 200494720.0000 - mae: 200494720.0000\n",
      "Epoch 37/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 202488608.0000 - mae: 202488608.0000\n",
      "Epoch 38/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 198242064.0000 - mae: 198242064.0000\n",
      "Epoch 39/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 198336608.0000 - mae: 198336608.0000\n",
      "Epoch 40/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 198982160.0000 - mae: 198982160.0000\n",
      "Epoch 41/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 197425728.0000 - mae: 197425728.0000\n",
      "Epoch 42/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 199484912.0000 - mae: 199484912.0000\n",
      "Epoch 43/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 199798288.0000 - mae: 199798288.0000\n",
      "Epoch 44/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 196860944.0000 - mae: 196860944.0000\n",
      "Epoch 45/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 194161616.0000 - mae: 194161616.0000\n",
      "Epoch 46/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 197629456.0000 - mae: 197629456.0000\n",
      "Epoch 47/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 191531024.0000 - mae: 191531024.0000\n",
      "Epoch 48/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 193772880.0000 - mae: 193772880.0000\n",
      "Epoch 49/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 194657184.0000 - mae: 194657184.0000\n",
      "Epoch 50/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 193878784.0000 - mae: 193878784.0000\n",
      "Epoch 51/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 194760000.0000 - mae: 194760000.0000\n",
      "Epoch 52/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 194729440.0000 - mae: 194729440.0000\n",
      "Epoch 53/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 189962928.0000 - mae: 189962928.0000\n",
      "Epoch 54/100\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 189746848.0000 - mae: 189746848.0000\n",
      "Epoch 55/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 189022176.0000 - mae: 189022176.0000\n",
      "Epoch 56/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 187215968.0000 - mae: 187215968.0000\n",
      "Epoch 57/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 185932336.0000 - mae: 185932336.0000\n",
      "Epoch 58/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 187055680.0000 - mae: 187055680.0000\n",
      "Epoch 59/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 188072256.0000 - mae: 188072256.0000\n",
      "Epoch 60/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 188322160.0000 - mae: 188322160.0000\n",
      "Epoch 61/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 189171424.0000 - mae: 189171424.0000\n",
      "Epoch 62/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 186957920.0000 - mae: 186957920.0000\n",
      "Epoch 63/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 189167248.0000 - mae: 189167248.0000\n",
      "Epoch 64/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 183908704.0000 - mae: 183908704.0000\n",
      "Epoch 65/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 182253392.0000 - mae: 182253392.0000\n",
      "Epoch 66/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 188609728.0000 - mae: 188609728.0000\n",
      "Epoch 67/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 187615248.0000 - mae: 187615248.0000\n",
      "Epoch 68/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 186163040.0000 - mae: 186163040.0000\n",
      "Epoch 69/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 183908656.0000 - mae: 183908656.0000\n",
      "Epoch 70/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 183343072.0000 - mae: 183343072.0000\n",
      "Epoch 71/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 183693456.0000 - mae: 183693456.0000\n",
      "Epoch 72/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 178635680.0000 - mae: 178635680.0000\n",
      "Epoch 73/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 181627312.0000 - mae: 181627312.0000\n",
      "Epoch 74/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 177086176.0000 - mae: 177086176.0000\n",
      "Epoch 75/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 181367056.0000 - mae: 181367056.0000\n",
      "Epoch 76/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 179951184.0000 - mae: 179951184.0000\n",
      "Epoch 77/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 178280976.0000 - mae: 178280976.0000\n",
      "Epoch 78/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 182626960.0000 - mae: 182626960.0000\n",
      "Epoch 79/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 179677744.0000 - mae: 179677744.0000\n",
      "Epoch 80/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 178126656.0000 - mae: 178126656.0000\n",
      "Epoch 81/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 176139120.0000 - mae: 176139120.0000\n",
      "Epoch 82/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 177882176.0000 - mae: 177882176.0000\n",
      "Epoch 83/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 181382224.0000 - mae: 181382224.0000\n",
      "Epoch 84/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 175273936.0000 - mae: 175273936.0000\n",
      "Epoch 85/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 177220400.0000 - mae: 177220400.0000\n",
      "Epoch 86/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 177666928.0000 - mae: 177666928.0000\n",
      "Epoch 87/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 173252960.0000 - mae: 173252960.0000\n",
      "Epoch 88/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 175585408.0000 - mae: 175585408.0000\n",
      "Epoch 89/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 177005024.0000 - mae: 177005024.0000\n",
      "Epoch 90/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 172937824.0000 - mae: 172937824.0000\n",
      "Epoch 91/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 175117344.0000 - mae: 175117344.0000\n",
      "Epoch 92/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 173194704.0000 - mae: 173194704.0000\n",
      "Epoch 93/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 174108512.0000 - mae: 174108512.0000\n",
      "Epoch 94/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 171788096.0000 - mae: 171788096.0000\n",
      "Epoch 95/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 172705088.0000 - mae: 172705088.0000\n",
      "Epoch 96/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 172476496.0000 - mae: 172476496.0000\n",
      "Epoch 97/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 172895536.0000 - mae: 172895536.0000\n",
      "Epoch 98/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 173900784.0000 - mae: 173900784.0000\n",
      "Epoch 99/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 170985120.0000 - mae: 170985120.0000\n",
      "Epoch 100/100\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 171699216.0000 - mae: 171699216.0000\n"
     ]
    }
   ],
   "source": [
    "# lets see if we can increase our current model by increaseing teh epochs to 100 \n",
    "tf.random.set_seed(42)\n",
    "# lets create a model using teh sequenctial API\n",
    "Apple_model= tf.keras.Sequential([ #groups a linear stack of layers into a tf.keras.Model.\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ]) # this is basically saying that we want to generate a model from keras \n",
    "# now we want to compule the model \n",
    "Apple_model.compile(loss=tf.keras.losses.mae, # mae measn mean abouslue error, which is a measure of error between paired observations expressing the same phenomenon, compairson between preducted vs observed  )- \n",
    "               optimizer=tf.keras.optimizers.SGD(),\n",
    "               metrics=[\"mae\"])\n",
    "# now we want to fit the model \n",
    "with tf.device('/cpu:0'): Apple_model.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100) # we need to use the with cup or the kernal will crash\n",
    "# epochs refers to the number of runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 10:50:21.087495: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[3.6777760e+07],\n",
       "        [3.6764412e+07],\n",
       "        [3.9643652e+07],\n",
       "        [3.9203764e+07],\n",
       "        [4.0783468e+07]],\n",
       "\n",
       "       [[1.9382378e+07],\n",
       "        [1.8276002e+07],\n",
       "        [2.0635380e+07],\n",
       "        [1.8489290e+07],\n",
       "        [1.9324348e+07]],\n",
       "\n",
       "       [[1.9172510e+08],\n",
       "        [1.9160702e+08],\n",
       "        [1.9208974e+08],\n",
       "        [1.9204595e+08],\n",
       "        [1.9488357e+08]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.2386928e+07],\n",
       "        [4.9974240e+07],\n",
       "        [5.4666324e+07],\n",
       "        [5.1440512e+07],\n",
       "        [5.2665640e+07]],\n",
       "\n",
       "       [[8.8460656e+07],\n",
       "        [8.8247392e+07],\n",
       "        [8.9990256e+07],\n",
       "        [8.9340432e+07],\n",
       "        [9.0924376e+07]],\n",
       "\n",
       "       [[1.9688956e+07],\n",
       "        [1.7969424e+07],\n",
       "        [2.1181890e+07],\n",
       "        [2.0515396e+07],\n",
       "        [2.0515396e+07]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organising the data \n",
    "- the data needs to be organised so we are able to process it using tensorflow \n",
    "- we need take in the 4 columns and then create a tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Strat:\n",
    "we want to create a parameter where we are able to see a large change(price swing) use the condiitons before and analyse the data for patterns whcih may give rise to a signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janadhi/miniforge3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/99 [=============>................] - ETA: 0s - loss: 4061439232.0000 - mae: 263303328.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 10:55:47.188197: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 2/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421456.0000\n",
      "Epoch 3/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421424.0000\n",
      "Epoch 4/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421456.0000\n",
      "Epoch 5/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421456.0000\n",
      "Epoch 6/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261440.0000 - mae: 263421504.0000\n",
      "Epoch 7/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421504.0000\n",
      "Epoch 8/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421440.0000\n",
      "Epoch 9/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421440.0000\n",
      "Epoch 10/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421552.0000\n",
      "Epoch 11/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421456.0000\n",
      "Epoch 12/100\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 4063262464.0000 - mae: 263421520.0000\n",
      "Epoch 13/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421456.0000\n",
      "Epoch 14/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263232.0000 - mae: 263421488.0000\n",
      "Epoch 15/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263232.0000 - mae: 263421504.0000\n",
      "Epoch 16/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n",
      "Epoch 17/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421424.0000\n",
      "Epoch 18/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421424.0000\n",
      "Epoch 19/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421456.0000\n",
      "Epoch 20/100\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 4063262976.0000 - mae: 263421504.0000\n",
      "Epoch 21/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 22/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421504.0000\n",
      "Epoch 23/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421440.0000\n",
      "Epoch 24/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421520.0000\n",
      "Epoch 25/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421456.0000\n",
      "Epoch 26/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421504.0000\n",
      "Epoch 27/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421520.0000\n",
      "Epoch 28/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421456.0000\n",
      "Epoch 29/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421424.0000\n",
      "Epoch 30/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421552.0000\n",
      "Epoch 31/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421488.0000\n",
      "Epoch 32/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n",
      "Epoch 33/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421424.0000\n",
      "Epoch 34/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421584.0000\n",
      "Epoch 35/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421488.0000\n",
      "Epoch 36/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261440.0000 - mae: 263421488.0000\n",
      "Epoch 37/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263232.0000 - mae: 263421488.0000\n",
      "Epoch 38/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n",
      "Epoch 39/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421520.0000\n",
      "Epoch 40/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421488.0000\n",
      "Epoch 41/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421456.0000\n",
      "Epoch 42/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421488.0000\n",
      "Epoch 43/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 44/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421520.0000\n",
      "Epoch 45/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421488.0000\n",
      "Epoch 46/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421456.0000\n",
      "Epoch 47/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421552.0000\n",
      "Epoch 48/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421552.0000\n",
      "Epoch 49/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421488.0000\n",
      "Epoch 50/100\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 4063262976.0000 - mae: 263421552.0000\n",
      "Epoch 51/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421440.0000\n",
      "Epoch 52/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 53/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421424.0000\n",
      "Epoch 54/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421440.0000\n",
      "Epoch 55/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421504.0000\n",
      "Epoch 56/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421504.0000\n",
      "Epoch 57/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421488.0000\n",
      "Epoch 58/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421568.0000\n",
      "Epoch 59/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421520.0000\n",
      "Epoch 60/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421520.0000\n",
      "Epoch 61/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421440.0000\n",
      "Epoch 62/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421488.0000\n",
      "Epoch 63/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421552.0000\n",
      "Epoch 64/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421440.0000\n",
      "Epoch 65/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421520.0000\n",
      "Epoch 66/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421488.0000\n",
      "Epoch 67/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421424.0000\n",
      "Epoch 68/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421504.0000\n",
      "Epoch 69/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421504.0000\n",
      "Epoch 70/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421504.0000\n",
      "Epoch 71/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421456.0000\n",
      "Epoch 72/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n",
      "Epoch 73/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421440.0000\n",
      "Epoch 74/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263232.0000 - mae: 263421504.0000\n",
      "Epoch 75/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421456.0000\n",
      "Epoch 76/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261696.0000 - mae: 263421440.0000\n",
      "Epoch 77/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421504.0000\n",
      "Epoch 78/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063260928.0000 - mae: 263421488.0000\n",
      "Epoch 79/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421376.0000\n",
      "Epoch 80/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421520.0000\n",
      "Epoch 81/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063261952.0000 - mae: 263421488.0000\n",
      "Epoch 82/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421504.0000\n",
      "Epoch 83/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421344.0000\n",
      "Epoch 84/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421440.0000\n",
      "Epoch 85/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421424.0000\n",
      "Epoch 86/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n",
      "Epoch 87/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421504.0000\n",
      "Epoch 88/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262720.0000 - mae: 263421504.0000\n",
      "Epoch 89/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421456.0000\n",
      "Epoch 90/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421488.0000\n",
      "Epoch 91/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063264000.0000 - mae: 263421504.0000\n",
      "Epoch 92/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421440.0000\n",
      "Epoch 93/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063263744.0000 - mae: 263421488.0000\n",
      "Epoch 94/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421440.0000\n",
      "Epoch 95/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063264256.0000 - mae: 263421520.0000\n",
      "Epoch 96/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262976.0000 - mae: 263421376.0000\n",
      "Epoch 97/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421520.0000\n",
      "Epoch 98/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 99/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421488.0000\n",
      "Epoch 100/100\n",
      "99/99 [==============================] - 0s 2ms/step - loss: 4063262464.0000 - mae: 263421504.0000\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "Apple_model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(4, activation = \"relu\"), \n",
    "    tf.keras.layers.Dense(1)\n",
    "   \n",
    "])\n",
    "Apple_model2.compile(loss = \"binary_crossentropy\", # dependent on the problem type \n",
    "                optimizer = tf.keras.optimizers.Adam(lr = 0.001),\n",
    "                metrics = [\"mae\"])\n",
    "\n",
    "with tf.device('/cpu:0'): history = Apple_model2.fit(X, y, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1ed3d0d51d0d9fc84e9feca67a2c96385eab8afcf16092b40709d9c6021dc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
